{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "fDrILIEWLt4B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve8QwFj0LjFD"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, [2, 3]]\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\"\"\" The shuffling is done with this method\"\"\"\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "  X, y, test_size=0.3, random_state=1, stratify=y)"
      ],
      "metadata": {
        "id": "iDsfnPmpMtss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sdandarization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "Ggr6lwKvMwei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the newly data\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, test_idx=None,resolution=0.02):\n",
        "  # setup marker generator and color map\n",
        "  markers = ('o', 's', '^', 'v', '<')\n",
        "  colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "  cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "  # plot the decision surface\n",
        "  x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "  x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "  xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),np.arange(x2_min, x2_max, resolution))\n",
        "  lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "  lab = lab.reshape(xx1.shape)\n",
        "  plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
        "  plt.xlim(xx1.min(), xx1.max())\n",
        "  plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "  # plot class examples\n",
        "  for idx, cl in enumerate(np.unique(y)):\n",
        "    plt.scatter(x=X[y == cl, 0],\n",
        "      y=X[y == cl, 1],\n",
        "      alpha=0.8,\n",
        "      c=colors[idx],\n",
        "      marker=markers[idx],\n",
        "      label=f'Class {cl}',\n",
        "      edgecolor='black')\n",
        "\n",
        "  # highlight test examples\n",
        "  if test_idx:\n",
        "    # plot all examples\n",
        "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1],\n",
        "      c='none', edgecolor='black', alpha=1.0,\n",
        "      linewidth=1, marker='o',\n",
        "      s=100, label='Test set')"
      ],
      "metadata": {
        "id": "72thFEY7M0wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees can build complex decision boundaries by dividing the feature space into rectangles.\n",
        "However, we have to be careful since the deeper the decision tree, the more complex the decision\n",
        "boundary becomes, which can easily result in overtting. Using scikit-learn, we will now train a de-\n",
        "cision tree with a maximum depth of 4, using the Gini impurity as a criterion for impurity."
      ],
      "metadata": {
        "id": "SJHaTMEzPFTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree_model = DecisionTreeClassifier(criterion='gini',max_depth=4,random_state=1)\n",
        "tree_model.fit(X_train, y_train)\n",
        "X_combined = np.vstack((X_train, X_test))\n",
        "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
        "y_combined = np.hstack((y_train, y_test))\n",
        "plot_decision_regions(X_combined,y_combined,classifier=tree_model,test_idx=range(105, 150))\n",
        "plt.xlabel('Petal length [cm]')\n",
        "plt.ylabel('Petal width [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GjDmy-ndNAar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A nice feature in scikit-learn is that it allows us to readily visualize the decision tree model aer train-\n",
        "ing via the following code:"
      ],
      "metadata": {
        "id": "qUBsC3OvQUxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "feature_names = ['Sepal length', 'Sepal width','Petal length', 'Petal width']\n",
        "tree.plot_tree(tree_model,feature_names=feature_names,filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kueU12gPPVyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble methods have gained huge popularity in applications of machine learning during the last\n",
        "decade due to their good classication performance and robustness toward overtting. While we\n",
        "are going to cover dierent ensemble methods, including bagging and boosting, later in Chapter 7,\n",
        "Combining Dierent Models for Ensemble Learning, let’s discuss the decision tree-based random forest\n",
        "algorithm, which is known for its good scalability and ease of use. A random forest can be considered\n",
        "as an ensemble of decision trees. The idea behind a random forest is to average multiple (deep) deci-\n",
        "sion trees that individually suer from high variance to build a more robust model that has a better\n",
        "generalization performance and is less susceptible to overtting. The random forest algorithm can\n",
        "be summarized in four simple steps:\n",
        "1.Draw a random bootstrap sample of size n (randomly choose n examples from the training\n",
        "dataset with replacement).\n",
        "2.Grow a decision tree from the bootstrap sample. At each node:\n",
        "a.Randomly select d features without replacement.\n",
        "b.Split the node using the feature that provides the best split according to the objective\n",
        "function, for instance, maximizing the information gain.\n",
        "3.Repeat steps 1-2 k times.\n",
        "4.Aggregate the prediction by each tree to assign the class label by majority vote"
      ],
      "metadata": {
        "id": "snXEJktuZQsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier(n_estimators=25,random_state=1,n_jobs=2)\n",
        "forest.fit(X_train, y_train)\n",
        "plot_decision_regions(X_combined, y_combined,classifier=forest, test_idx=range(105,150))\n",
        "plt.xlabel('Petal length [cm]')\n",
        "plt.ylabel('Petal width [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W7oaef1KQUIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the preceding code, we trained a random forest from 25 decision trees via the n_estimators\n",
        "parameter. By default, it uses the Gini impurity measure as a criterion to split the nodes. Although\n",
        "we are growing a very small random forest from a very small training dataset, we used the n_jobs pa-\n",
        "rameter for demonstration purposes, which allows us to parallelize the model training using multiple\n",
        "cores of our computer (here, two cores). If you encounter errors with this code, your computer may\n",
        "not support multiprocessing. You can omit the n_jobs parameter or set it to n_jobs=None."
      ],
      "metadata": {
        "id": "h-O1dUznZ1cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-nearest neighbors"
      ],
      "metadata": {
        "id": "Wohg2GQOah68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5, p=2,metric='minkowski')\n",
        "knn.fit(X_train_std, y_train)\n",
        "plot_decision_regions(X_combined_std, y_combined,classifier=knn, test_idx=range(105,150))\n",
        "plt.xlabel('Petal length [standardized]')\n",
        "plt.ylabel('Petal width [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JaY3M8B1ZwJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FL4qhJJxgGiX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}